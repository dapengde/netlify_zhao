<meta charset="utf-8">

**Regression**
 Dr. Peng Zhao






# Ice breaking

- Role play: - *Manga Guide to Statistics: Correlation*

# Objectives

1. Explore a relationship between two variables.
2. Work with regression.
3. Visualize linear regression.

# Prediction

Demo: I would like to predict an ENV203 student's total mark. What is the best prediction for a student I do not know?

- The mean from the marks of previous years.


~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
dtf <- data.frame(
  id = 1:16,
  env = c(56, 74, 56, 81, 75, 84, 68, 52, 57, 82, 73, 90, 67, 79, 70, 66))
~~~~~~~~~~~~~~~~~~~~~~~~~~~


| id| env|
|--:|---:|
|  1|  56|
|  2|  74|
|  3|  56|
|  4|  81|
|  5|  75|
|  6|  84|
|  7|  68|
|  8|  52|
|  9|  57|
| 10|  82|
| 11|  73|
| 12|  90|
| 13|  67|
| 14|  79|
| 15|  70|
| 16|  66|


~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
(mean_env <- mean(dtf$env))
~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~
## [1] 70.625
~~~~~~~~~~~~~~~~~~~~~~~~~~~

If I know more information about the students...


~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
dtf$gaokao <- c(45, 81, 65, 87, 68, 91, 77, 61, 55, 66, 82, 93, 76, 83, 61, 74)
~~~~~~~~~~~~~~~~~~~~~~~~~~~


| id| env| gaokao|
|--:|---:|------:|
|  1|  56|     45|
|  2|  74|     81|
|  3|  56|     65|
|  4|  81|     87|
|  5|  75|     68|
|  6|  84|     91|
|  7|  68|     77|
|  8|  52|     61|
|  9|  57|     55|
| 10|  82|     66|
| 11|  73|     82|
| 12|  90|     93|
| 13|  67|     76|
| 14|  79|     83|
| 15|  70|     61|
| 16|  66|     74|

...a more accurate prediction becomes possible. I can predict which students will pass ENV203 and which ones will not!


~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
plot(x = dtf$gaokao, y = dtf$env)
mean_gaokao <- mean(dtf$gaokao)
points(x = mean_gaokao, y = mean_env, col = "gray", cex = 3)
abline(h = mean_env, v = mean_gaokao, col = "gray")
~~~~~~~~~~~~~~~~~~~~~~~~~~~

![](figure/unnamed-chunk-7-1.png)

Generally, a student with high Gaokao score gets a high score in ENV203. 

Then, if we know a student's Gaokao score, can we predict her ENV203 score better than using the mean?

# Linear regression line

Draw a line to summarize the relationship in the scatter plot.

We can draw many many lines. Which one best summarizes the relationship?

![](figure/unnamed-chunk-8-1.png)

$$y' = a + bx$$

$x, y'$
: A point on the line. $y'$ is the ordinate, and $x$ is the abscissa, $y'$ is the best prediction of $y$ for a given value of $x$.

$a, b$
: The regression coefficients. $a$ is the intercept, i.e. the point where the line crosses the vertical axis, and $b$ is the slope.


The regression line
:    The best fitting line, which passes through the maximum number of points and isn't too far away from the points that it does not pass through. It means, the distances in the vertical direction between the points is a minimum --- the minimal sum of the square of these distances ($\sum(y'-y)^2$). The Least Squares Line. The sum of the squared vertical deviations of the observed data points from the least-squares line is smaller than the sum of the squared vertical deviations of the data points from any other line.


$$b = \frac{\sum(x-\bar x)(y -\bar y)}{\sum(x-\bar x)^2}$$

$$a = \bar y - b\bar x$$

!!! note: Exercise
    Calculate $a$ and $b$ using these two equations above. Give the prediction equation.
    


# Variation around the regression line

Residuals
:  Deviations of the observed $y$-values around the predicted  $y$-values ($y-y'$, residual)

Residual variance
:    roughly an average of the squared residuals:

    $$s^2_{yx} = \frac{\sum(y-y')^2}{N-2}$$

Residual standard error
:    $s_{yx}$.

!!! note: Exercise
    Fill in the following table. Calculate the residual standard error.
    

| id| env| gaokao|predicted |residual |
|--:|---:|------:|:---------|:--------|
|  1|  56|     45|          |         |
|  2|  74|     81|          |         |
|  3|  56|     65|          |         |
|  4|  81|     87|          |         |
|  5|  75|     68|          |         |
|  6|  84|     91|          |         |
|  7|  68|     77|          |         |
|  8|  52|     61|          |         |
|  9|  57|     55|          |         |
| 10|  82|     66|          |         |
| 11|  73|     82|          |         |
| 12|  90|     93|          |         |
| 13|  67|     76|          |         |
| 14|  79|     83|          |         |
| 15|  70|     61|          |         |
| 16|  66|     74|          |         |



The smaller the residual standard error is, the better the fit is. 

But how small?


# Simple linear regression model

In simple linear regression (SLR), two variables, usually labeled *X* and *Y* , are of interest. The variable *X* is the independent variable and the variable *Y* is the dependent variable.

The regression process so described is termed “regressing *Y* on *X*”.

Assumptions:

1. Values of *X* are "fixed" and non-random.

2. *X* is measured without error.

3. For each value of *X* there is a subpopulation of values of *Y* that are normally distributed.

4. The variances of the subpopulations of *Y* are are equal and denoted by $\sigma^2$. This is known as the **assumption of homoskedasticity**.

5. The means of the subpoulations of *Y* all lie on the same straight line. This is known as the **assumption of linearity** and is expressed as

   $\mu_{y|x} = \alpha + \beta x$

6. The *Y* values are statistically independent.

The SLR model is represented by the following equation:

$y' = \alpha + \beta x +\epsilon$

$\epsilon$ is called the error term. Solving for $\epsilon$, we get $\epsilon = y' - (\alpha + \beta x) = y' - \mu_{y|x} $ . This indicates that the error term is the amount by which an observation *y* deviates from the mean of the subpopulation of *Y* values from which it is drawn.

Variables according to purpose

- Independent variables are input or cause variables
  Synonyms: predictor variable, regressor, controlled variable, manipulated variable, explanatory variable, exposure variable, risk factor, feature and input variable.

- Dependent variables are output or effect variables
  Synonyms: Response variable, regressand, measured variable, responding variable, explained variable, outcome variable, experimental variable, and output variable

# Hypothesis test about fit

Demo: How well does the regression line fit the scatter plot?

1. Hypotheses
  - $H_0$: There is no relationship between the Gaokao scores and the ENV203 scores.
  - $H_1$: There is relationship between the Gaokao scores and the ENV203 scores.
  - Question: Reject $H_0$?

2. Collect data.

3. Calculate a test statistic. $F$-test.

![](https://pzhao.org/img/concept_linear_regression_test.jpg)

The distance between the point and the mean ($y - \bar y$) is the sum of the distance between the point and the regression line ($y-y'$, residual) and the distance between the regression line and the mean ($y'-\bar y$):

$$y - \bar y = (y-y') - (y'-\bar y)$$

$\sum(y-y')^2$
:    The sum of squared residual. $SS_\mathrm{residual}$.

$\sum(y'-\bar y)^2$
:    The gain in prediction due to using the regression line rather than using the mean. $SS_\mathrm{regression}$.

$\sum(y -\bar y)^2$
:    The numerator of the variance of $y$. The numerator of the total variance. $SS_\mathrm{total}$.

$$SS_\mathrm{residual} + SS_\mathrm{regression} = SS_\mathrm{total}$$

$$df_\mathrm{residual} + df_\mathrm{regression} = df_\mathrm{total}$$

$$df_\mathrm{regression} = 1$$

$$df_\mathrm{total} = N - 1$$
 
$$df_\mathrm{residual} = N - 2$$

| Source                                    | $df$           | $SS$                                                         | $MS$                        | $F$                     |
| ----------------------------------------- | -------------- | ------------------------------------------------------------ | --------------------------- | ----------------------- |
| between the point and the mean            | $df_\mathrm{total} = N - 1$    | $SS_\mathrm{total} = \sum(y -\bar y)^2$      | $MS_\mathrm{total} = \frac{SS_\mathrm{total}}{df_\mathrm{total}}$   | $F = \frac{MS_\mathrm{regression}}{MS_\mathrm{residual}}$ |
| between the point and the regression line | $df_\mathrm{residual} = N - 2$ | $SS_\mathrm{residual} = \sum(y-y')^2$        | $MS_\mathrm{residual} = \frac {SS_\mathrm{residual}}{df_\mathrm{residual}}$ |                         |
| between the regression line and the mean  | $df_\mathrm{regression} = 1$   | $SS_\mathrm{regression} = \sum(y'-\bar y)^2$ | $MS_\mathrm{regression} = \frac{SS_\mathrm{regression}}{df_\mathrm{regression}}$ |                         |

- If $H_0$ is true (i.e. no relationship between $x$ and $y$), $MS_\mathrm{regression}$ should be no greater than $MS_\mathrm{residual}$.
- If $H_0$ is false (i.e. relationship between $x$ and $y$), $MS_\mathrm{regression}$ should be much greater than $MS_\mathrm{residual}$.

New hypotheses:

  - $H_0: \sigma ^2_\mathrm{regression} \le \sigma ^2_\mathrm{residual}$
  - $H_1: \sigma ^2_\mathrm{regression} > \sigma ^2_\mathrm{residual}$
  - $F$-test

!!! note: Exersice
    Fill in the following table.

| Source                                    | $df$  | $SS$   | $MS$  | $F$  |
| ----------------------------------------- | ----- | ------ | ----- | ---- |
| between the point and the mean            |       |        |       |      |
| between the point and the regression line |       |        |       |      |
| between the regression line and the mean  |       |        |       |      |




4. Decision.

# Hypothesis test about the slope

1. Hypotheses:
  - $H_0: \beta = 0$. The slope of the regression line is zero.
  - $H_1: \beta \ne 0$. The slope of the regression line is not zero.

2. Collect data

3. Calculate a test statistic. $t$-test.

$$t = \frac{b - \beta}{s_b}$$
$$s_b = \frac{s_{yx}}{s_x \sqrt{N-1}}$$

!!! note: Exercise
    Calculate the $t$ score for the slope of the Gaokao-Env203 regression line.

4. Decision.

# Hypothesis test about the intercept

1. Hypotheses:
  - $H_0: \alpha = 0$. The intercept of the regression line is zero.
  - $H_1: \alpha \ne 0$. The intercept of the regression line is not zero.

2. Collect data

3. Calculate a test statistic. $t$-test.

$$t = \frac{a - \alpha}{s_a}$$
$$s_a = s_{yx}\sqrt{\frac{1}{N} + \frac{\bar x^2}{(N-1)s_x^2}}$$

!!! note: Exercise
    Calculate the $t$ score for the intercept of the Gaokao-Env203 regression line.

4. Decision.

# One-step in R


~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
env_lm <- lm(env ~ gaokao, data = dtf)
env_lm
~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 
## Call:
## lm(formula = env ~ gaokao, data = dtf)
## 
## Coefficients:
## (Intercept)       gaokao  
##     23.0299       0.6537
~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
summary(env_lm)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~
## 
## Call:
## lm(formula = env ~ gaokao, data = dtf)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.9036  -5.3720  -0.4379   4.2111  15.8281 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  23.0299    10.2732   2.242 0.041697 *  
## gaokao        0.6537     0.1389   4.707 0.000337 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.249 on 14 degrees of freedom
## Multiple R-squared:  0.6128,	Adjusted R-squared:  0.5851 
## F-statistic: 22.15 on 1 and 14 DF,  p-value: 0.0003368
~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Visualization



~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
plot(dtf$gaokao, dtf$env)
abline(lm_env)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

![](figure/unnamed-chunk-14-1.png)



~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
library(beginr)
plotlm(dtf$gaokao, dtf$env, refline = TRUE)
~~~~~~~~~~~~~~~~~~~~~~~~~~~

![](figure/unnamed-chunk-15-1.png)

~~~~~~~~~~~~~~~~~~~~~~~~~~~
## [[1]]
##              Estimate Std. Error  t value     Pr(>|t|)
## (Intercept) 23.029869  10.273158 2.241752 0.0416970916
## x            0.653667   0.138878 4.706772 0.0003368488
## 
## [[2]]
## [1] 0.6127645
~~~~~~~~~~~~~~~~~~~~~~~~~~~


~~~~~~~~~~~~~~~~~~~~~~~~~~~r linenumbers
library(ggplot2)
ggplot(dtf, aes(gaokao, env)) +
  geom_point() +
  geom_smooth(method = "lm")
~~~~~~~~~~~~~~~~~~~~~~~~~~~

![](figure/unnamed-chunk-16-1.png)

# Assignments

!!!note: Readings
    
    - Simple Linear Regression
    - Data Regression

!!! note: plant height
    A scientist assumes that the temperature (the explanatory variable) is a good predictor of plant height (the response variable), and the logarithm of the plant height linearly depends on the temperature. She collects a data set as Plant_height.csv.
    
    1. Download the data from the Learning Mall.
    2. Make a scatter plot of the logarithm of the plant height against the temperature. Mark the means.
    3. Apply the simple linear regression model and get the intercept and slope for the regression line.
    4. Do the hypothesis test about the fit step by step.
    5. Do the hypothesis test about the slope step by step.
    6. Do the hypothesis test about the intercept step by step.
    7. Visualize the simple linear regression in the graph.
    
<!-- Markdeep js: -->
<script src="D:/Program Files/R/R-4.0.1/library/deepdown/js/deepdown-doc.js" charset="utf-8"></script>
<script>markdeepOptions={inlineCodeLang:"R", tocStyle:"short"};</script>

<!-- FALLBACK: -->
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>

<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>

<!-- Inverse the color: -->
<style>
.md .inverse svg.diagram {
  background: #333;
  stroke: #FFF;
  fill: #FFF;
}

.md .inverse svg.diagram .opendot {
  fill: #333;
}
</style>

<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://pzhao.org/js/deepdown-doc.js?" charset="utf-8"></script>
